{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMU+L9xggWKvv3JPsCkbgDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p1kalys/own_neural_network/blob/main/Neural_Network_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==2.4.1 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTd9xrObkZeD",
        "outputId": "5cb5fc5b-f729-4548-c59f-f8e7ea85ed2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yGC_f__LlHma",
        "outputId": "48d809b1-d9bd-454c-9111-8b41e2430ed5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL3heMw4qDgw",
        "outputId": "0eac08df-923a-499a-8f48-e1c1d656d8e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor0d = torch.tensor(1)\n",
        "print(\"D-0: \",tensor0d)\n",
        "\n",
        "# create a 1D tensor (vector) from a Python list\n",
        "tensor1d = torch.tensor([1, 2, 3])\n",
        "print(\"D-1: \",tensor1d)\n",
        "# create a 2D tensor from a nested Python list\n",
        "tensor2d = torch.tensor([[1, 2], [3, 4]])\n",
        "print(\"D-2: \",tensor2d)\n",
        "\n",
        "# create a 3D tensor from a nested Python list\n",
        "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "print(\"D-3: \",tensor3d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA9IqZ3KqD6Y",
        "outputId": "45a622b9-cac3-4eb5-ebb8-a0ea261034e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D-0:  tensor(1)\n",
            "D-1:  tensor([1, 2, 3])\n",
            "D-2:  tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "D-3:  tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry3_bXDvquOM",
        "outputId": "0196afa4-eb3b-4916-e33f-1e3621647f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(floatvec.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkG6OhFvq4jG",
        "outputId": "9690588d-f680-4af0-df45-844e0caf84f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "floatvec = tensor1d.to(torch.float32)\n",
        "print(floatvec.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmxqREJhq78d",
        "outputId": "201858fc-7693-41e1-bcc4-343f77c43d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d = torch.tensor([[1, 2, 3],\n",
        "                         [4, 5, 6]])\n",
        "print(tensor2d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdHc4RdyrNQT",
        "outputId": "b4b51a1c-cb23-434b-ba25-e0f1246116ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d.reshape(3, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b277kB7PrRTu",
        "outputId": "f3d7aae5-c6d9-48e4-a41e-446531b7eec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# common command for reshaping tensors in PyTorch is .view():\n",
        "\n",
        "tensor2d.view(3, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiWr4SPJrqmr",
        "outputId": "c1568402-a6ea-4ee8-83ac-e0a761af65d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use .T to transpose a tensor\n",
        "\n",
        "tensor2d.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIMEjscpr55m",
        "outputId": "cace9fd4-cf49-44f8-b591-c03715ba6fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 4],\n",
              "        [2, 5],\n",
              "        [3, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# common way to multiply two matrices in PyTorch is the .matmul method:\n",
        "\n",
        "tensor2d.matmul(tensor2d.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQmmeTPKsB2H",
        "outputId": "6237e3b6-e6c4-4622-8e49-059f4e085e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14, 32],\n",
              "        [32, 77]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also adopt the @ operator like matmul\n",
        "tensor2d @ tensor2d.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MwFmz9dsJcz",
        "outputId": "50f80ec1-58c9-4c5d-f217-262bf31ad8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14, 32],\n",
              "        [32, 77]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "y = torch.tensor([1.0])  # true label\n",
        "x1 = torch.tensor([1.1]) # input feature\n",
        "w1 = torch.tensor([2.2], requires_grad=True) # weight parameter\n",
        "b = torch.tensor([0.0], requires_grad=True)  # bias unit\n",
        "\n",
        "z = x1 * w1 + b          # net input\n",
        "a = torch.sigmoid(z)     # activation & output\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLCHAi_2sUto",
        "outputId": "edcaceaf-9600-46f4-c3be-7c1379ac769c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0852, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import grad\n",
        "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
        "grad_L_b = grad(loss, b, retain_graph=True)"
      ],
      "metadata": {
        "id": "vGqEblIhtoc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grad_L_w1)\n",
        "print(grad_L_b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga_rkjvlxVY3",
        "outputId": "840e033c-f63e-4076-e9b9-03425f365d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([-0.0898]),)\n",
            "(tensor([-0.0817]),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "B9VU9jaDxoc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w1.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTMLidKBxvLa",
        "outputId": "18682d53-0928-4f44-94ec-90fb3743a2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0898])\n",
            "tensor([-0.0817])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc51f402",
        "outputId": "f3e26c6d-47c3-4fbb-8787-31f2fd89a965"
      },
      "source": [
        "# Without using Sequential as it is not necessarily required here but if we have layers involved\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNetworkWithoutSequential(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(num_inputs, 30)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(30, 20)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(20, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "# Example usage:\n",
        "model_without_sequential = NeuralNetworkWithoutSequential(50, 3)\n",
        "print(model_without_sequential)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetworkWithoutSequential(\n",
            "  (fc1): Linear(in_features=50, out_features=30, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=30, out_features=20, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=20, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_inputs, 30),\n",
        "            torch.nn.ReLU(), #Rectified Linear Unit - makes negative values to zero leading to sparse activation in the network\n",
        "\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(30, 20),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(20, num_outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.layers(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "4wjCYx3nxy1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(50, 3)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6Mr19b1Dy7",
        "outputId": "193dcc88-bbc6-4c5f-d6a5-4c3bbbe60c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad\n",
        ")\n",
        "print(\"Total number of trainable model parameters:\", num_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhKCn2Tv3QTY",
        "outputId": "a1772bf9-4092-4e53-d9c2-3f1b2ef04d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable model parameters: 2213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A linear layer multiplies the inputs with a weight matrix and adds a bias vector. This is sometimes also referred to as a feedforward or fully connected layer."
      ],
      "metadata": {
        "id": "RAsxsRam3swx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[0].weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBq1rE2m3YIq",
        "outputId": "3f39d4f4-7755-404a-8393-27891f7e1744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.1357, -0.1411, -0.0157,  ..., -0.0692, -0.0799, -0.0207],\n",
            "        [ 0.0666,  0.1302,  0.0685,  ...,  0.0968, -0.0752,  0.0351],\n",
            "        [-0.0802, -0.1361,  0.0539,  ..., -0.1296,  0.0247, -0.0573],\n",
            "        ...,\n",
            "        [ 0.1235,  0.0483,  0.0386,  ..., -0.0942,  0.0949, -0.0408],\n",
            "        [ 0.1177, -0.0269, -0.0773,  ...,  0.0744, -0.0667, -0.0955],\n",
            "        [ 0.0086, -0.1050, -0.0910,  ..., -0.0594, -0.1144,  0.0400]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[0].weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Bl2W4s3y_o",
        "outputId": "78378333-63a4-4561-8c41-b3c3c45a75dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqVxxUNB35h7",
        "outputId": "9edab3a5-0e6c-42d3-868b-66d34fbbe126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([ 0.1014, -0.0131, -0.1255,  0.0782,  0.0786,  0.1243,  0.0448,  0.1084,\n",
              "         0.1056,  0.1405,  0.0816, -0.0499,  0.1338,  0.0451, -0.0388,  0.1116,\n",
              "         0.0058,  0.1338, -0.1137,  0.1013, -0.0623,  0.1363,  0.1142,  0.0899,\n",
              "        -0.1043, -0.0662, -0.0292, -0.0204,  0.1398, -0.0449],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to keep using small random numbers as initial values for our layer weights, we can make the random number initialization reproducible by seeding PyTorch’s random number generator via manual_seed"
      ],
      "metadata": {
        "id": "8KLPVQED4OZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "model = NeuralNetwork(50, 3)\n",
        "print(model.layers[0].weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx40PVrS39N8",
        "outputId": "92ab14e5-5062-4148-86b2-77afaa0a1227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
            "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
            "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
            "        ...,\n",
            "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
            "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
            "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "X = torch.rand((1, 50))\n",
        "out = model(X)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2ULYJOI4ThU",
        "outputId": "a2379799-3dbb-456e-dbf2-fc192a03a3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <AddmmBackward0> part of grad_fn=<AddmmBackward0> specifies the operation that was performed. In this case, it is an Addmm operation. Addmm stands for matrix multiplication (mm) followed by an addition (Add)."
      ],
      "metadata": {
        "id": "IJRylVsJ4-4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we just want to use a network without training or backpropagation, for example, if we use it for prediction after training, constructing this computational graph for backpropagation can be wasteful as it performs unnecessary computations and consumes additional memory. So, when we use a model for inference (for instance, making predictions) rather than training, it is a best practice to use the torch.no_grad() context manager, as shown below."
      ],
      "metadata": {
        "id": "s6SkYW1_5puz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    out = model(X)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAP62m3V4kTc",
        "outputId": "b1929a08-9f71-40d5-d077-03f682516263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1262,  0.1080, -0.1792]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, it’s common practice to code models such that they return the outputs of the last layer (logits) without passing them to a nonlinear activation function. That’s because PyTorch’s commonly used loss functions combine the softmax (or sigmoid for binary classification) operation with the negative log-likelihood loss in a single class. The reason for this is numerical efficiency and stability. So, if we want to compute class-membership probabilities for our predictions, we have to call the softmax function explicitly"
      ],
      "metadata": {
        "id": "2dTO-Iop5exY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    out = torch.softmax(model(X), dim=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBrJfZ245Xlh",
        "outputId": "58575870-aa2b-4520-bbc9-bb118ba79877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3113, 0.3934, 0.2952]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor([\n",
        "    [-1.2, 3.1],\n",
        "    [-0.9, 2.9],\n",
        "    [-0.5, 2.6],\n",
        "    [2.3, -1.1],\n",
        "    [2.7, -1.5]\n",
        "])\n",
        "\n",
        "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
        "\n",
        "X_test = torch.tensor([\n",
        "    [-0.8, 2.8],\n",
        "    [2.6, -1.6],\n",
        "])\n",
        "\n",
        "y_test = torch.tensor([0, 1])"
      ],
      "metadata": {
        "id": "7IjZQLLW6f4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class label numbering PyTorch requires that class labels start with label 0, and the largest class label value should not exceed the number of output nodes minus 1"
      ],
      "metadata": {
        "id": "eve4F2MW6u9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        one_x = self.features[index]\n",
        "        one_y = self.labels[index]\n",
        "        return one_x, one_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "train_ds = ToyDataset(X_train, y_train)\n",
        "test_ds = ToyDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "00GS7Tsm5asU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCAZPjNp6bsS",
        "outputId": "cafa50d6-8171-4028-98a8-58a736d0e04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "_mAcSjM866mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = ToyDataset(X_test, y_test)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "HJDsQ9-j7RUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we specified a batch size of 2 above, but the 3rd batch only contains a single example. That’s because we have five training examples, which is not evenly divisible by 2. In practice, having a substantially smaller batch as the last batch in a training epoch can disturb the convergence during training. To prevent this, it’s recommended to set drop_last=True, which will drop the last batch in each epoch.\n",
        "\n",
        "When num_workers is set to 0, the data loading will be done in the main process and not in separate worker processes. This might seem unproblematic, but it can lead to significant slowdowns during model training when we train larger networks on a GPU. This is because instead of focusing solely on the processing of the deep learning model, the CPU must also take time to load and preprocess the data. As a result, the GPU can sit idle while waiting for the CPU to finish these tasks. In contrast, when num_workers is set to a number greater than zero, multiple worker processes are launched to load data in parallel, freeing the main process to focus on training your model and better utilizing your system’s resources"
      ],
      "metadata": {
        "id": "mGh2bIFj8LVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {idx+1}:\", x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaY7jssL7eVa",
        "outputId": "4117fc28-d67c-4ff3-8200-6f86e64e49c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: tensor([[ 2.3000, -1.1000],\n",
            "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
            "Batch 2: tensor([[-1.2000,  3.1000],\n",
            "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
            "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "        logits = model(features)\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels) # Loss function\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        ### LOGGING\n",
        "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
        "              f\" | Train/Val Loss: {loss:.2f}\")\n",
        "\n",
        "    model.eval()\n",
        "    # Optional model evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6EC6ovz7thh",
        "outputId": "f0a5b516-db14-44a1-85fb-641a3ac79b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/003 | Batch 000/003 | Train/Val Loss: 0.75\n",
            "Epoch: 001/003 | Batch 001/003 | Train/Val Loss: 0.65\n",
            "Epoch: 001/003 | Batch 002/003 | Train/Val Loss: 0.42\n",
            "Epoch: 002/003 | Batch 000/003 | Train/Val Loss: 0.05\n",
            "Epoch: 002/003 | Batch 001/003 | Train/Val Loss: 0.13\n",
            "Epoch: 002/003 | Batch 002/003 | Train/Val Loss: 0.00\n",
            "Epoch: 003/003 | Batch 000/003 | Train/Val Loss: 0.01\n",
            "Epoch: 003/003 | Batch 001/003 | Train/Val Loss: 0.00\n",
            "Epoch: 003/003 | Batch 002/003 | Train/Val Loss: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preventing undesired gradient accumulation. It is important to include an optimizer.zero_grad() call in each update round to reset the gradients to zero. Otherwise, the gradients will accumulate, which may be undesired.  we pass the logits directly into the cross_entropy loss function, which will apply the softmax function internally for efficiency and numerical stability reasons. Then, calling loss.backward() will calculate the gradients in the computation graph that PyTorch constructed in the background. The optimizer.step() method will use the gradients to update the model parameters to minimize the loss"
      ],
      "metadata": {
        "id": "XbM18rfx_BbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_train)\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61bXJkvz9D-1",
        "outputId": "f6ea1f03-5b95-44ed-e539-5681e18244cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.9320, -4.2563],\n",
            "        [ 2.6045, -3.8389],\n",
            "        [ 2.1484, -3.2514],\n",
            "        [-2.1461,  2.1496],\n",
            "        [-2.5004,  2.5210]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False) # The set_printoptions call is used here to make the outputs more legible.\n",
        "probas = torch.softmax(outputs, dim=1)\n",
        "print(probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHG_AgIh_TuA",
        "outputId": "9714db28-4543-4d6f-b8ac-1788503571cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0.9992,     0.0008],\n",
            "        [    0.9984,     0.0016],\n",
            "        [    0.9955,     0.0045],\n",
            "        [    0.0134,     0.9866],\n",
            "        [    0.0066,     0.9934]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can convert these values into class labels predictions using PyTorch’s argmax function, which returns the index position of the highest value in each row if we set dim=1 (setting dim=0 would return the highest value in each column, instead)"
      ],
      "metadata": {
        "id": "07mCLbnf_y4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.argmax(probas, dim=1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ccuHEaf_YEJ",
        "outputId": "c1821069-e1e9-4e94-d0ad-44664fc6346e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that it is unnecessary to compute softmax probabilities to obtain the class labels. We could also apply the argmax function to the logits (outputs) directly"
      ],
      "metadata": {
        "id": "lLfuMxAc_77g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.argmax(outputs, dim=1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uiMydBI_2VC",
        "outputId": "f6ed643a-987c-4379-9557-a79fced12997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(predictions == y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KBDWYAf_-zW",
        "outputId": "938ae41a-4fa2-49c1-97f5-2808f41b2089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, dataloader):\n",
        "\n",
        "    model = model.eval()\n",
        "    correct = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    for idx, (features, labels) in enumerate(dataloader):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(features)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        compare = labels == predictions\n",
        "        correct += torch.sum(compare)\n",
        "        total_examples += len(compare)\n",
        "\n",
        "    return (correct / total_examples).item()"
      ],
      "metadata": {
        "id": "z6IgEyL1AFeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_accuracy(model, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFZHAIJ2AJej",
        "outputId": "3c642060-a1aa-4d1c-b898-49c90bd186ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_accuracy(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VscMNfsQAcTf",
        "outputId": "87f32de0-fea6-433e-e881-f1bc98318db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "JPpZiYd0AoOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(2, 2) # needs to match the original model exactly\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7cnaMz5AryX",
        "outputId": "cb7ab22e-27e5-45f4-ec3f-afcf1073484c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktMuKfQHAx5G",
        "outputId": "74ae0112-ce1e-4118-c2d6-2a050388599c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1 = torch.tensor([1., 2., 3.])\n",
        "tensor_2 = torch.tensor([4., 5., 6.])\n",
        "\n",
        "print(tensor_1 + tensor_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glcp2DF6A1Cs",
        "outputId": "31ec0bf5-0687-4091-da3c-bfad0d72b071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5., 7., 9.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1 = tensor_1.to(\"cuda\")\n",
        "tensor_2 = tensor_2.to(\"cuda\")\n",
        "\n",
        "print(tensor_1 + tensor_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_Wem1wAA4Fj",
        "outputId": "628ea558-36b8-4e27-d0d6-6f56386e837c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5., 7., 9.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "\n",
        "# New: Define a device variable that defaults to a GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "# New: Transfer the model onto the GPU.\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "        # New: Transfer the data onto the GPU.\n",
        "        features, labels = features.to(device), labels.to(device)    #C\n",
        "        logits = model(features)\n",
        "        loss = F.cross_entropy(logits, labels) # Loss function\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        ### LOGGING\n",
        "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
        "              f\" | Train/Val Loss: {loss:.2f}\")\n",
        "\n",
        "    model.eval()\n",
        "    # Optional model evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuSgTnk5A8xB",
        "outputId": "d6e806eb-c3f6-4b82-e6e4-84903f872a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/003 | Batch 000/003 | Train/Val Loss: 0.75\n",
            "Epoch: 001/003 | Batch 001/003 | Train/Val Loss: 0.65\n",
            "Epoch: 001/003 | Batch 002/003 | Train/Val Loss: 0.42\n",
            "Epoch: 002/003 | Batch 000/003 | Train/Val Loss: 0.05\n",
            "Epoch: 002/003 | Batch 001/003 | Train/Val Loss: 0.13\n",
            "Epoch: 002/003 | Batch 002/003 | Train/Val Loss: 0.00\n",
            "Epoch: 003/003 | Batch 000/003 | Train/Val Loss: 0.01\n",
            "Epoch: 003/003 | Batch 001/003 | Train/Val Loss: 0.00\n",
            "Epoch: 003/003 | Batch 002/003 | Train/Val Loss: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fEP6u-FiBOKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# NEW imports:\n",
        "import os\n",
        "import platform\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "\n",
        "# NEW: function to initialize a distributed process group (1 process / GPU)\n",
        "# this allows communication among processes\n",
        "def ddp_setup(rank, world_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        rank: a unique process ID\n",
        "        world_size: total number of processes in the group\n",
        "    \"\"\"\n",
        "    # Only set MASTER_ADDR and MASTER_PORT if not already defined by torchrun\n",
        "    if \"MASTER_ADDR\" not in os.environ:\n",
        "        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "    if \"MASTER_PORT\" not in os.environ:\n",
        "        os.environ[\"MASTER_PORT\"] = \"12345\"\n",
        "\n",
        "    # initialize process group\n",
        "    if platform.system() == \"Windows\":\n",
        "        # Disable libuv because PyTorch for Windows isn't built with support\n",
        "        os.environ[\"USE_LIBUV\"] = \"0\"\n",
        "        # Windows users may have to use \"gloo\" instead of \"nccl\" as backend\n",
        "        # gloo: Facebook Collective Communication Library\n",
        "        init_process_group(backend=\"gloo\", rank=rank, world_size=world_size)\n",
        "    else:\n",
        "        # nccl: NVIDIA Collective Communication Library\n",
        "        init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        one_x = self.features[index]\n",
        "        one_y = self.labels[index]\n",
        "        return one_x, one_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_inputs, 30),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(30, 20),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(20, num_outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.layers(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def prepare_dataset():\n",
        "    X_train = torch.tensor([\n",
        "        [-1.2, 3.1],\n",
        "        [-0.9, 2.9],\n",
        "        [-0.5, 2.6],\n",
        "        [2.3, -1.1],\n",
        "        [2.7, -1.5]\n",
        "    ])\n",
        "    y_train = torch.tensor([0, 0, 0, 1, 1])\n",
        "\n",
        "    X_test = torch.tensor([\n",
        "        [-0.8, 2.8],\n",
        "        [2.6, -1.6],\n",
        "    ])\n",
        "    y_test = torch.tensor([0, 1])\n",
        "\n",
        "    # Uncomment these lines to increase the dataset size to run this script on up to 8 GPUs:\n",
        "    # factor = 4\n",
        "    # X_train = torch.cat([X_train + torch.randn_like(X_train) * 0.1 for _ in range(factor)])\n",
        "    # y_train = y_train.repeat(factor)\n",
        "    # X_test = torch.cat([X_test + torch.randn_like(X_test) * 0.1 for _ in range(factor)])\n",
        "    # y_test = y_test.repeat(factor)\n",
        "\n",
        "    train_ds = ToyDataset(X_train, y_train)\n",
        "    test_ds = ToyDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_ds,\n",
        "        batch_size=2,\n",
        "        shuffle=False,  # NEW: False because of DistributedSampler below\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "        # NEW: chunk batches across GPUs without overlapping samples:\n",
        "        sampler=DistributedSampler(train_ds)  # NEW\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_ds,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# NEW: wrapper\n",
        "def main(rank, world_size, num_epochs):\n",
        "\n",
        "    ddp_setup(rank, world_size)  # NEW: initialize process groups\n",
        "\n",
        "    train_loader, test_loader = prepare_dataset()\n",
        "    model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "    model.to(rank)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "    model = DDP(model, device_ids=[rank])  # NEW: wrap model with DDP\n",
        "    # the core model is now accessible as model.module\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # NEW: Set sampler to ensure each epoch has a different shuffle order\n",
        "        train_loader.sampler.set_epoch(epoch)\n",
        "\n",
        "        model.train()\n",
        "        for features, labels in train_loader:\n",
        "\n",
        "            features, labels = features.to(rank), labels.to(rank)  # New: use rank\n",
        "            logits = model(features)\n",
        "            loss = F.cross_entropy(logits, labels)  # Loss function\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # LOGGING\n",
        "            print(f\"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "                  f\" | Batchsize {labels.shape[0]:03d}\"\n",
        "                  f\" | Train/Val Loss: {loss:.2f}\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    try:\n",
        "        train_acc = compute_accuracy(model, train_loader, device=rank)\n",
        "        print(f\"[GPU{rank}] Training accuracy\", train_acc)\n",
        "        test_acc = compute_accuracy(model, test_loader, device=rank)\n",
        "        print(f\"[GPU{rank}] Test accuracy\", test_acc)\n",
        "\n",
        "    ####################################################\n",
        "    # NEW:\n",
        "    except ZeroDivisionError as e:\n",
        "        raise ZeroDivisionError(\n",
        "            f\"{e}\\n\\nThis script is designed for 2 GPUs. You can run it as:\\n\"\n",
        "            \"torchrun --nproc_per_node=2 DDP-script-torchrun.py\\n\"\n",
        "            f\"Or, to run it on {torch.cuda.device_count()} GPUs, uncomment the code on lines 103 to 107.\"\n",
        "        )\n",
        "    ####################################################\n",
        "\n",
        "    destroy_process_group()  # NEW: cleanly exit distributed mode\n",
        "\n",
        "\n",
        "def compute_accuracy(model, dataloader, device):\n",
        "    model = model.eval()\n",
        "    correct = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    for idx, (features, labels) in enumerate(dataloader):\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(features)\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        compare = labels == predictions\n",
        "        correct += torch.sum(compare)\n",
        "        total_examples += len(compare)\n",
        "    return (correct / total_examples).item()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # NEW: Use environment variables set by torchrun if available, otherwise default to single-process.\n",
        "    if \"WORLD_SIZE\" in os.environ:\n",
        "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "    else:\n",
        "        world_size = 1\n",
        "\n",
        "    if \"LOCAL_RANK\" in os.environ:\n",
        "        rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "    elif \"RANK\" in os.environ:\n",
        "        rank = int(os.environ[\"RANK\"])\n",
        "    else:\n",
        "        rank = 0\n",
        "\n",
        "    # Only print on rank 0 to avoid duplicate prints from each GPU process\n",
        "    if rank == 0:\n",
        "        print(\"PyTorch version:\", torch.__version__)\n",
        "        print(\"CUDA available:\", torch.cuda.is_available())\n",
        "        print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    num_epochs = 3\n",
        "    main(rank, world_size, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALi3CIzIcOCf",
        "outputId": "6eb32050-1481-439e-e03a-26ad4d2efcca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "Number of GPUs available: 1\n",
            "[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.66\n",
            "[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.33\n",
            "[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.10\n",
            "[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.07\n",
            "[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.04\n",
            "[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.01\n",
            "[GPU0] Training accuracy 1.0\n",
            "[GPU0] Test accuracy 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OtjOvOOWdzRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}